<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<title>NTTData4 - lab virtual assistant v2</title>
		<link rel="stylesheet" href="assets/css/base.sass">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
	</head>
	<body>
		<h1>NTTData4 - lab virtual assistant v2
			<div>Project website</div>
		</h1>
		<div class="nav">
			<a href="javascript:window.scrollTo(0,0)">home</a>
			<a href="#requirements">requirements</a>
			<a href="#research">research</a>
			<a href="#ui-design">ui design</a>
			<a href="#system-design">system design</a>
			<a href="#implementation">implementation</a>
			<a href="#testing">testing</a>
			<a href="#evaluation">evaluation</a>
			<a href="#appendices">appendices</a>
			<a href="https://comp0016-team-24.github.io/">blog</a>
		</div>
		<script src="assets/shadow-control.js"></script>

		<h3>Project Title: Lab Virtual Assistant v2</h3>
		<!-- TODO: update with project abstract later -->
		<p>
			For this project, we worked with NTTDATA to build an improved version of <a href="https://students.cs.ucl.ac.uk/2019/group24/index.html">last year's team 24 project</a>, a lab virtual assistant. It is a set of components which together allows a user to talk to an animated avatar backed by Alexa. We build upon their code to make the assistant more engaging to interact with, more configurable, as well as improving existing installation process and adding extra features.
		</p>

		<!-- TODO: need to talk about the solution + achievement and impact -->

		<p>
			We also have a third year student&mdash;Brandon Tan&mdash;working on this project at the same time. Our team and the third year student
			focus on different tasks, and we will list requirements that are not ours separately.
		</p>

		<h3>Project Video</h3>
		<iframe class="video-aspect-fit" style="width: 100%; height: 400px;" src="https://www.youtube-nocookie.com/embed/C-20sJ5b8uc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

		<h3>Development Team</h3>
		<!-- TODO: add roles of team members -->
		<div class="team">
			<div class="left">
				<img src="https://www.gravatar.com/avatar/6dd7d517f1b93e4a5b7688d11dbb31f4?size=200&d=identicon" alt="photo" style="width: 200px;height: 200px; margin-top: 1em; border-radius: 10px;">
				<h3>Tingmao Wang </h3><a href="mailto:tingmao.wang.19@ucl.ac.uk"><img src="assets/images/mail.png" class="member-icon"></a>
				<a href="https://github.com/micromaomao"><img src="assets/images/github-logo.png" class="member-icon">
			</div>
			<div class="middle">
				<img src="https://www.gravatar.com/avatar/bb862c9de0e640ac79de684283ab464a?size=200&d=identicon" alt="photo" style="width: 200px;height: 200px; margin-top: 1em; border-radius: 10px">
				<h3>Victoria Xiao</h3><a href="mailto:victoria.xiao.19@ucl.ac.uk"><img src="assets/images/mail.png" class="member-icon"></a>
				<a href="https://github.com/victoriax01"><img src="assets/images/github-logo.png" class="member-icon">
			</div>
			<div class="right">
				<img src="https://www.gravatar.com/avatar/58056c0f55794be43ddfa806b0b9ee4f?size=200&d=identicon" alt="photo" style="width: 200px;height: 200px; margin-top: 1em; border-radius: 10px">
				<h3>Kaloyan Rusev</h3><a href="mailto:kaloyan.rusev.19@ucl.ac.uk"><img src="assets/images/mail.png"class="member-icon"></a>
				<a href="https://github.com/kalcho100"><img src="assets/images/github-logo.png" class="member-icon">
			</div>
		</div>

		<h3>Project management</h3>

		<h4>Gantt Chart</h4>
		<img src="assets/images/gantt_chart_24032021.png" alt="gantt chart" style="width:90%;height:90%">

		<h2 id="requirements">Requirements</h2>

		<h3>Project Background and Client Introduction</h3>

		<p>
			The virtual assistant project is about building a digital avatar that is to be displayed on a lab TV/screen. Visitors
			and employees in the lab can interact with the assistant via voice commands. The assistant should be able to give the user an introduction to the lab and
			the company, and be able to handle different queries about the lab or the company.
		</p>

		<p>
			For example, if the company organizes a VR workshop in the lab, after being led to the lab by reception, visitors should be able to ask the
			assistant about what happens next, where to go, etc. and possibly also have the assistant help demonstrate some of the VR features in the lab.
		</p>

		<!-- TODO: add client introduction (NTTDATA is...) -->

		<h3>Project Goal</h3>

		<p>The goal of our project is to improve the existing solution and make the assistant more professional, engaging and be able to perform more tasks.</p>

		<h3>Requirement gathering</h3>

		<p>
			In our first meeting with the client, we asked about the requirements and were given some ideas of what we could achieve throughout this project. In the following meetings, we clarified and agreed on the requirements. The client occasionally added requirements throughout the project timeline.
		</p>

		<h3>Personas</h3>
		<h4>Peter Jensen</h4>
		<img src="assets/images/persona1.jpg" alt="persona1" style="width:75%;height:75%;">
		<p>
			Peter Jensen is a technology enthusiast who is attending a workshop at NTTDATA. He uses the assistant to find out what room his workshop is in and receive directions to get there. He is happy he is able to talk with the assistant naturally.
		</p>

		<h4>Joseph Richardson</h4>
		<img src="assets/images/persona2.jpg" alt="persona2" style="width:75%;height:75%">
		<p>
			Joseph Richardson is an employee who manages the assistant. He is not familiar with complex programming so he wants a system that is easy to set up, configure and maintain. He found the lab assistant's set up to be very straightforward.
		</p>

		<h3>Use case diagram</h3>
		<img src="assets/images/use_case.png" alt="use_case_diagram" style="width:50%;height:50%;">

		<h3>Use case list</h3>
		<h4>Lab visitor</h4>
		<ul>
			<li>
				Interact with the assistant using natural language
			</li>
			<li>
				Ask for information about the company
			</li>
			<li>
				Ask for a video to be shown about the company
			</li>
			<li>
				Ask for directions
			</li>
			<li>
				Change the language of the assistant
			</li>
		</ul>
		<h4>System manager</h4>
		<ul>
			<li>
				Interact with the assistant using natural language
			</li>
			<li>
				Set up the assistant on their own systems, with both portrait and landscape being possible
			</li>
			<li>
				Configure the navigation system to add and specify directions to rooms
			</li>
			<li>
				Ask for usage information about the assistant
			</li>
			<li>
				Configure IoT integration
			</li>
		</ul>

		<h3>MoSCoW requirement list</h3>
		<!-- TODO: need to modify to specify which are functional & which are non-functional -->
		<h4>Must Have</h4>
		<ul>
			<li>
				We must implement a <b>better background</b>
				<p>
					Currently the avatar shows an empty background. We plan to implement a feature to match the background to the current weather.
				</p>
			</li>
			<li>
				We must implement a <b>navigation system</b>
				<p>
					Visitors to the lab should be able to ask the assistant where to go for a certain place or event, and the assistant
					should be able to show a map, along with voice directions.
				</p>
			</li>
			<li>
				We must <b>simplify the installation</b>
				<p>
					Currently, setting up the assistant involves the deployment of multiple moving pieces (see <a href="#system-design"># System Design</a>).
					Ideally this should be improved to a one-click install and run.
				</p>
			</li>
			<li>
				We must have a <b>configurable 3D model</b>
				<p>
					Currently the assistant only has one 3D form. We aim to be able to support using multiple models, so that
					the company or the user can select the 3D avatar they like the most.
				</p>
			</li>
			<li>
				We must support <b>portrait orientation</b>
				<p>
					Currently the assistant only supports landscape mode. The assistant must be able to adjust accordingly to both landscape and portrait monitors/TV screens.
				</p>
			</li>
		</ul>
		<h4>Should Have</h4>
		<ul>
			<li>
				We should implement an interface to allow <b>reporting</b>
				<p>
					The client suggested a monitoring interface to show usage of the assistant.
				</p>
			</li>
			<li>
				We should have <b>IoT integration</b>
				<p>
					This would involve making the assistant able to control building lights, etc.
				</p>
			</li>
		</ul>

		<h4>Could Have</h4>

		<h4>Won't Have</h4>

		<h4>Brandon's requirements</h4>
		<ul>
			<li>
				Lip sync
				<p>
					One of Brandon's major requirement, to make the mouth show the correct shape when Alexa is speaking.
				</p>
			</li>
			<li>
				Emotions / expressions
				<p>
					Also one of Brandon's area, to make the avatar able to convey appropiate emotions to the user via facial features.
				</p>
			</li>
			<li>
				Keeping user engaged while waiting for a response
				<p>
					This could involve the use of some loading animation
				</p>
			</li>
			<li>
				Translation / act as a translator
			</li>
			<li>
				More responsive AI
			</li>
			<li>
				Display user input
			</li>
		</ul>

		<h2 id="research">Research</h2>

		<!-- <h3>Related Project Review</h3>
		<p>
			We found a project, <a href="https://www.loomi.ai/">Loomi</a>, which is similar to our project as it is a virtual personal assistant. The project is in closed beta so we were unable to test the project but
		</p> -->

		<h3>Automated installation tool</h3>

		<p>
			We originally attempted to make the project runnable with just one installer which automatically supports both the Unity client and the Alexa client. We researched about how to make that possible.
		</p>

		<p>
			For example, since we use Python, we wanted to "package" our python code such that user doesn't need to have Python installed or install any pip dependencies themselves. We found <a href="https://www.pyinstaller.org/index.html">PyInstaller</a> which seemed like a viable option. It packages the application along with its dependencies and Python into a standalone executable, which is easy to install and run.
		</p>

		<p>
			However, not all dependencies will work, especially if the Python package relies on native code that needs to be compiled. There is a <a href="https://github.com/pyinstaller/pyinstaller/wiki/Supported-Packages">list of supported packages</a>, but it doesn't include pyaudio, for example. We did not attempt to build it since we decided to go with another approach later on.
		</p>

		<h2 id="ui-design">UI Design</h2>

		<h3>Design Principles</h3>
		<!-- TODO: add design principles (e.g. simplicity, consistency, visibility, feedback, tolerance, etc.) -->

		<h3>Sketches</h3>
		<p>
			After gathering our requirements, we created hand-drawn sketches to explore the ways we could design the interface to integrate the design principles (visibility, feedback, affordance, consistency, constraints). Originally, we had 2 sets of sketches. After some user feedback, we decided to use these as our final sketches.
		</p>

		<img src="assets/images/sketch_config.jpg" alt="sketch_config" style="height:250px;">
		<img src="assets/images/sketch_captions.jpg" alt="sketch_captions" style="height:250px;">
		<img src="assets/images/sketch_video.jpg" alt="sketch_video" style="height:250px;">

		<h3>Interactive Prototype Video</h3>
		<p>
			Using Balsamiq Cloud, we created an interactive prototype to show our design ideas and the interaction between the assistant and the user, here is the video. The speech bubbles represent voice commands from the user. (Note: the info button on the config page was added after the prototype evaluation)
		</p>
		<iframe class="video-aspect-fit" width="560" height="315" src="https://www.youtube-nocookie.com/embed/9dLzVgGIIl8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		<img src="assets/images/speech_bubbles.JPG" alt="speech_bubbles" style="height:315px;width: 560px;">
		<h3>Prototype Evaluation</h3>
		<p>We used an analytic evaluation by evaluating our prototype through heuristics.</p>
		<table style="width:100%">
			<tr>
				<th>Location</th>
				<th>Heuristic</th>
				<th>Problem</th>
				<th>Solution</th>
				<th>Severity</th>
			</tr>
			<tr>
				<td>Configuration page ​</td>
				<td>Help and documentation</td>
				<td>There is no guidance on how to setup the configuration page.</td>
				<td>Create a help popup in the configuration page</td>
				<td>3</td>
			</tr>
			<tr>
				<td>Location</td>
				<td>Visibility of system status</td>
				<td>Once a voice command is inputted, there is no indication if the system is taking a while to respond or if it has crashed.</td>
				<td>Have an animated thought bubble above avatar to represent that system is processing.</td>
				<td>2</td>
			</tr>
			<tr>
				<td>Location</td>
				<td>Recognition rather than recall</td>
				<td>User may forget how to activate or ask for some specific things from the assistant.</td>
				<td>Display suggested questions if a user is detected but there is no input.</td>
				<td>2</td>
			</tr>
		</table>
		<h3>References</h3>
		<p>
			Preece, J., Sharp, H., & Rogers, Y. (2019). Interaction design: beyond human-computer interaction, Wiley, 5th Edition Section 1.7.3
		</p>

		<h2 id="system-design">System Design</h2>

		<p>
			The decision to use Alexa is made by the previous year's student and we decided to build upon that. The system architecture had gone through some modification in order to simplify the use of our system.
		</p>

		<img src="assets/images/system-arch.svg" alt="System Architecture Diagram" class="sysarch">

		<p>
			As seen above, this project contains 3 separate but connected components, which together supports our Alexa integration. Essentially, user's queries are listened and sent by the Alexa client to the Alexa voice service to be processed by Amazon. If the user uses our skill, the Alexa voice service will send requests over https to our skill server, which will process the request and send the text response back to Alexa. It would also send control messages to the Unity front-end (which is the "UI" of this system) to facilitate video playing, lip sync, navigation, etc.
		</p>

		<p>
			Unlike previously, the Alexa client now also runs on Windows, which means that running a Linux virtual machine on the client's Windows screen is no longer necessary. The Alexa client can run on the same operating system as the unity front-end.
		</p>

		<h2 id="implementation">Implementation</h2>

		<h3>Alexa integration</h3>

		<ul>
			<li>
				<p>
					The skill server is a Flask (Python) http(s) server. We used a "HTTP polling" approach (due to websocket library not working properly) for sending messages to Unity. Unity will periodically request <code>/msg</code>, and the server will return the current state, which consists of what the current response is, whether Unity should play video, etc. as well as the timestamp of when the state was updated last.
				</p>
			</li>
			<li>
				<p>
					For the Alexa client, we pulled the source of the <a href="https://github.com/alexa/avs-device-sdk">official C++ Alexa client</a> and slightly modified it to suit our system:
				</p>
				<ul>
					<li>
						We changed it to output mp3 files instead of playing the Alexa response.
					</li>
					<li>
						We changed it to take input from a temporary file instead of stdin, so that our Python script can control it.
					</li>
					<li>
						We changed it to output Alexa response to a temporary file, so that our Python script can pass the text response onto Unity to do lip-sync.
					</li>
				</ul>
				<p>
					To communicate with Unity, we also wrote a Python script which is supposed to be run alongside the client, which starts up a local http server and use the same polling approach used in the skill server to let Unity get messages from it.
				</p>
				<p>
					The official Alexa client does not contain hot word detection (recognizing when the user calls "Alexa" and hence start listening for query), so this is implemented by using an third-party library in the python script, and asking Alexa to start listening when we detected user saying "Alexa".
				</p>
			</li>
			<li>
				<p>
					Finally, the Unity client uses basic C# code to talk with the two components above via HTTP(S).
				</p>
			</li>
		</ul>

		<h3>Installation</h3>

		<p>
			To simplify skill server setup, we wrote a completely automated configuration script, which will build a Docker container with all the dependencies, get certificates from Let's Encrypt, and setup systemd services which will start the skill server on boot and allow it to run in background, removing the need for user to run "screen" themselves.
		</p>

		<p>
			To simplify the client installation (Unity front-end and Alexa client), we pre-compiled everything that we can and packaged them as zip files, and linked to those packages in our user manual. This means that user doesn't need to have Unity installed, and they wouldn't need to wait for the Alexa client to compile either.
		</p>

		<h3>Configuration</h3>

		<p>
			Configuration for the Unity front-end are read from a JSON file. We wrote a html page to generate such JSON file with a UI for some configuration changes, so that user doesn't need to manually modify JSON themselves.
		</p>

		<h3>Adaptive Background based on the Weather</h3>

        <p>
            One of our requirements was to improve the background. The client suggested a background that shows a different weather state depending on the real time weather.
        </p>
        <p>
            The implementation of this mainly involves Unity scripts and calling a Weather API to receive information about the live weather. There is a <a href="https://docs.unity3d.com/ScriptReference/Coroutine".html>coroutine</a> in weatherControl.cs which calls the <a href="https://openweathermap.org/api">weather api</a> using <a href="https://docs.unity3d.com/ScriptReference/Networking.UnityWebRequest.html">UnityWebRequests</a>. Then using <a href="https://github.com/Bunny83/SimpleJSON">SimpleJSON</a> to parse the JSON from the API, the system knows the weather.
        </p>
        <p>
            There are many different weather states so the script groups them into 4 states: Clear, Clouds, Rain and Snow. Clouds, Rain and Snow share the same skybox of a cloudy background. Rain and Snow use <a href="https://docs.unity3d.com/ScriptReference/ParticleSystem.html">particle systems</a> to create their visual effects. Clear uses its own skybox with a clear sky.
        </p>
        <p>
            The main Update() method of the script calls the coroutine every 10 minutes since the API is limited to 60 calls per minute and 1,000,000 calls per month.
        </p>

		<h2 id="testing">Testing</h2>

		<p>
			To ensure reliability, we wrote automated unit tests for the skill using our test Alexa client and a Unity unit test which uses the Unity test runner.
		</p>

		<h2 id="evaluation">Evaluation</h2>
		<h3>Summary of achievements</h3>
		<h4>MoSCoW Table</h4>
		<table style="width:100%">
			<tr>
				<th>ID</th>
				<th>Requirement</th>
				<th>Priority</th>
				<th>State</th>
				<th>Contributors</th>
			</tr>
			<tr>
				<td>1​</td>
				<td>Improved Background</td>
				<td>Must</td>
				<td>✓</td>
				<td>Victoria</td>
			</tr>
			<tr>
				<td>2</td>
				<td>Navigation System</td>
				<td>Must</td>
				<td>✓</td>
				<td>Kaloyan</td>
			</tr>
			<tr>
				<td>3</td>
				<td>Simplify Installation</td>
				<td>Must</td>
				<td>✓</td>
				<td>Tingmao</td>
			</tr>
			<tr>
				<td>4</td>
				<td>Configurable 3D model</td>
				<td>Must</td>
				<td>✓</td>
				<td>Tingmao</td>
			</tr>
			<tr>
				<td>5</td>
				<td>Portrait Mode Support</td>
				<td>Must</td>
				<td>✓</td>
				<td>Victoria, Tingmao</td>
			</tr>
			<tr>
				<td>6</td>
				<td>Menu Screen</td>
				<td>Must</td>
				<td>✓</td>
				<td>Kaloyan</td>
			</tr>
			<tr>
				<td>7</td>
				<td>Add branding</td>
				<td>Should</td>
				<td>✓</td>
				<td>Victoria</td>
			</tr>
			<tr>
				<td>8</td>
				<td>Improved Face Tracking</td>
				<td>Should</td>
				<td>✓</td>
				<td>Victoria</td>
			</tr>
			<tr>
				<td>9</td>
				<td>Improved Video Player</td>
				<td>Should</td>
				<td>✓</td>
				<td>Tingmao</td>
			</tr>
			<tr>
				<td>10​</td>
				<td>Reporting</td>
				<td>Should</td>
				<td>X</td>
				<td></td>
			</tr>
			<tr>
				<td>11​</td>
				<td>IoT Integration</td>
				<td>Could</td>
				<td>X</td>
				<td></td>
			</tr>
			<tr>
				<td>1​2</td>
				<td>Change Language</td>
				<td>Could</td>
				<td>X</td>
				<td></td>
			</tr>
			<tr>
				<th colspan="2">Key Functionalities (must have and should have)</th>
				<td colspan="3"><b>90%</b> completed</td>
			</tr>
			<tr>
				<th colspan="2">Optional Functionalities (could have)</th>
				<td colspan="3"><b>0%</b> completed</td>
			</tr>
		</table>

		<h4>Known Bug List</h4>
		<!-- TODO: bug list -->
		<table style="width:100%">
			<tr>
				<th>ID</th>
				<th>Bug Description</th>
				<th>Priority</th>
			</tr>
			<tr>
				<td>1</td>
				<td>Alexa client is not always able to maintain session, causing user to need to say "Open Blue Assistant" again or "Ask Blue Assistant &hellip;" all the time.</td>
				<td>High</td>
			</tr>
			<tr>
				<td>2​</td>
				<td>Face detection often fails if the lighting where the user's face is located is too dark</td>
				<td>Low</td>
			</tr>
		</table>

		<h4>Individual Contribution Distribution Table</h4>
		<table style="width:100%">
			<tr>
				<th>Work packages</th>
				<th>Tingmao</th>
				<th>Kaloyan</th>
				<th>Victoria</th>
			</tr>
			<tr>
				<th>Client liaison​</th>
				<td>30%</td>
				<td>30%</td>
				<td>40%</td>
			</tr>
			<tr>
				<th>Requirement analysis</th>
				<td>33%</td>
				<td>33%</td>
				<td>34%</td>
			</tr>
			<tr>
				<th>Research</th>
				<td>30%</td>
				<td>50%</td>
				<td>20%</td>
			</tr>
			<tr>
				<th>UI design</th>
				<td>10%</td>
				<td>40%</td>
				<td>50%</td>
			</tr>
			<tr>
				<th>Programming</th>
				<td>55%</td>
				<td>30%</td>
				<td>15%</td>
			</tr>
			<tr>
				<th>Testing</th>
				<td>100%</td>
				<td>0%</td>
				<td>0%</td>
			</tr>
			<tr>
				<th>Development Blog</th>
				<td>34%</td>
				<td>33%</td>
				<td>33%</td>
			</tr>
			<tr>
				<th>Report Website Editing</th>
				<td>40%</td>
				<td>20%</td>
				<td>40%</td>
			</tr>
			<tr>
				<th>Video Editing</th>
				<td>0%</td>
				<td>0%</td>
				<td>100%</td>
			</tr>
			<tr>
				<th>Overall Contribution</th>
				<td><b>34%</b></td>
				<td><b>33%</b></td>
				<td><b>33%</b></td>
			</tr>
			<tr>
				<th>Main Roles</th>
				<td><b>Backend developer, Tester, Researcher</b></td>
				<td><b>Frontend developer, Researcher</b></td>
				<td><b>Frontend developer, UI designer, Report editor</b></td>
			</tr>
		</table>

		<h3>Critical Evaluation</h3>

		<!-- TODO: critical evaluation of the project: user interface/experience, functionality, stability, efficiency, compatibility, maintainability, project management -->
		<h3>Future work</h3>
		<ul>
			<li>
				Better lip sync
				<p>
					The lip synchronisation is still not perfect, it assumes that each syllable takes the same amount of time so sometimes, the assistant’s lip synchronisation becomes quite out of sync with the voice output. If there was more time, this could be improved.
				</p>
			</li>
			<li>
				Moving off Alexa
				<p>
					The project could be developed to move off Alexa which would allow for more control over the assistant – such as having a custom hotword and not requiring the user to open our skill to use the custom voice commands. The hotword detection is also slightly inconsistent, sometimes requiring the user to say ‘Alexa’ more than once for it to trigger but with a custom made voice controlled client, this can be more tailored.
				</p>
			</li>
			<li>
				Installers
				<p>
					Although we have simplified the installation, it could still be further simplified – our current solution still requires the user to install some dependencies such as Python, Java and msys2. Perhaps an installer could be created for the project to automatically carry out the steps documented in the user manual.
				</p>
			</li>

		</ul>

		<h2 id="appendices">Appendices</h2>

		<h3>User manual</h3>

		<a href="user-manual.pdf">Open PDF</a>&nbsp;&nbsp;or&nbsp;&nbsp;<a href="user-manual.htm">View as HTML</a>

		<h3 id="legal">Legal</h3>
		<p>
			<b>The software is an early proof of concept for development purposes and should not be used as-is in a live environment without further redevelopment and/or testing.</b> No warranty is given and no real data or personally identifiable data should be stored. Usage and its liabilities are your own.
		</p>
		<p>
			<a href="./license-details.html">See a list of open source libraries &amp; assets used</a>
		</p>
		<p>
			For ease of development, this project provides pre-compiled binaries made with Unity, usage of which are subject to restrictions in the <a href="https://unity3d.com/legal/terms-of-service/software">Unity Software Additional Terms</a>. This is not legal advice.
		</p>

		<div class="footer">
			<div class="left">
				This is a <a href="https://www.ucl.ac.uk/module-catalogue/modules/systems-engineering/COMP0016">UCL Computer Science :: COMP0016</a> coursework.<br>
				Website bundled with Parcel. <a href="https://github.com/COMP0016-Team-24/project-website">Source code</a>
			</div>
			<div class="right">
				2nd Year team members:<br>
				<ul>
					<li>Tingmao <a href="mailto:tingmao.wang.19@ucl.ac.uk">tingmao.wang.19@ucl.ac.uk</a></li>
					<li>Victoria <a href="mailto:victoria.xiao.19@ucl.ac.uk">victoria.xiao.19@ucl.ac.uk</a></li>
					<li>Kaloyan <a href="mailto:kaloyan.rusev.19@ucl.ac.uk">kaloyan.rusev.19@ucl.ac.uk</a></li>
				</ul>
			</div>
		</div>

		<script src="assets/video-aspect-fit.js"></script>
	</body>
</html>
